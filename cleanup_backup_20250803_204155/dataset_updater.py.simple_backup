import os
import pandas as pd
import numpy as np
import pickle
import json
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import catboost as cb
from werkzeug.utils import secure_filename
import shutil

class DatasetUpdater:
    def __init__(self):
        # Use notebooks/ml_model as primary model storage location
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.models_path = os.path.join(project_root, 'notebooks', 'ml_model')
        self.backup_path = os.path.join(project_root, 'model_backups')
        
        # Ensure directories exist
        os.makedirs(self.models_path, exist_ok=True)
        os.makedirs(self.backup_path, exist_ok=True)
        
        self.tanah_columns = [
            'kecamatan', 'NJOP_Rp_per_m2', 'Sertifikat', 'Sewa_per_Bulan',
            'luas_tanah_m2', 'Jenis_zona', 'Aksesibilitas', 'Tingkat_Keamanan',
            'Kepadatan_Penduduk'
        ]
        
        self.bangunan_columns = [
            'Kecamatan', 'Kamar Tidur', 'Kamar Mandi', 'Luas Tanah (m¬≤)',
            'Luas Bangunan (m¬≤)', 'Sertifikat', 'Daya Listrik (watt)',
            'Kondisi Perabotan', 'Jumlah Lantai', 'Hadap', 'Terjangkau Internet',
            'Sumber Air', 'Hook', 'Kondisi Properti', 'Aksesibilitas',
            'NJOP (Rp/m¬≤)', 'Tingkat Keamanan', 'Sewa per Bulan (Rp)', 'Jenis Zona'
        ]

    def backup_existing_models(self, dataset_type):
        """Backup existing models before replacing them"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = os.path.join(self.backup_path, f"{dataset_type}_{timestamp}")
        os.makedirs(backup_dir, exist_ok=True)
        
        model_files = [
            f"{dataset_type}_random_forest_model.pkl",
            f"{dataset_type}_xgboost_model.pkl", 
            f"{dataset_type}_catboost_model.pkl",
            f"{dataset_type}_ensemble_model.pkl",
            f"preprocessor_{dataset_type}.pkl",
            f"feature_names_{dataset_type}.pkl"
        ]
        
        backed_up_files = []
        for model_file in model_files:
            source_path = os.path.join(self.models_path, model_file)
            if os.path.exists(source_path):
                dest_path = os.path.join(backup_dir, model_file)
                shutil.copy2(source_path, dest_path)
                backed_up_files.append(model_file)
        
        return backup_dir, backed_up_files

    def validate_dataset(self, df, dataset_type):
        """Validate dataset format and required columns"""
        required_columns = self.tanah_columns if dataset_type == 'tanah' else self.bangunan_columns
        
        # Check for required columns
        missing_columns = []
        for col in required_columns:
            # Check both exact match and case-insensitive match
            if col not in df.columns:
                # Try to find similar column names
                similar_cols = [c for c in df.columns if col.lower() in c.lower() or c.lower() in col.lower()]
                if not similar_cols:
                    missing_columns.append(col)
        
        if missing_columns:
            return False, f"Missing required columns: {', '.join(missing_columns)}"
        
        # Check for minimum data requirements
        if len(df) < 10:
            return False, "Dataset must contain at least 10 records"
        
        # Check for target column
        target_col = 'Sewa_per_Bulan' if dataset_type == 'tanah' else 'Sewa per Bulan (Rp)'
        if target_col not in df.columns:
            return False, f"Target column '{target_col}' not found"
        
        return True, "Dataset validation passed"

    def preprocess_data(self, df, dataset_type):
        """Preprocess the dataset for training"""
        try:
            # Make a copy to avoid modifying original
            df = df.copy()
            
            if dataset_type == 'tanah':
                # Standardize column names for tanah
                column_mapping = {
                    'kecamatan': 'Kecamatan',
                    'NJOP_Rp_per_m2': 'NJOP_per_m2',
                    'luas_tanah_m2': 'Luas_Tanah',
                    'Sewa_per_Bulan': 'Sewa_per_Bulan',
                    'Jenis_zona': 'Jenis_Zona',
                    'Tingkat_Keamanan': 'Tingkat_Keamanan'
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns:
                        df[new_col] = df[old_col]
                
                # Select features for tanah
                feature_cols = ['Kecamatan', 'NJOP_per_m2', 'Sertifikat', 'Luas_Tanah',
                               'Jenis_Zona', 'Aksesibilitas', 'Tingkat_Keamanan', 'Kepadatan_Penduduk']
                target_col = 'Sewa_per_Bulan'
                
            else:  # bangunan
                # Standardize column names for bangunan
                column_mapping = {
                    'Luas Tanah (m¬≤)': 'Luas_Tanah',
                    'Luas Bangunan (m¬≤)': 'Luas_Bangunan', 
                    'Kamar Tidur': 'Kamar_Tidur',
                    'Kamar Mandi': 'Kamar_Mandi',
                    'Daya Listrik (watt)': 'Daya_Listrik',
                    'Kondisi Perabotan': 'Kondisi_Perabotan',
                    'Jumlah Lantai': 'Jumlah_Lantai',
                    'Terjangkau Internet': 'Terjangkau_Internet',
                    'Sumber Air': 'Sumber_Air',
                    'Kondisi Properti': 'Kondisi_Properti',
                    'NJOP (Rp/m¬≤)': 'NJOP_per_m2',
                    'Tingkat Keamanan': 'Tingkat_Keamanan',
                    'Sewa per Bulan (Rp)': 'Sewa_per_Bulan',
                    'Jenis Zona': 'Jenis_Zona'
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns:
                        df[new_col] = df[old_col]
                
                # Select features for bangunan
                feature_cols = ['Kecamatan', 'Kamar_Tidur', 'Kamar_Mandi', 'Luas_Tanah', 'Luas_Bangunan',
                               'Sertifikat', 'Daya_Listrik', 'Kondisi_Perabotan', 'Jumlah_Lantai', 'Hadap',
                               'Terjangkau_Internet', 'Sumber_Air', 'Hook', 'Kondisi_Properti', 
                               'Aksesibilitas', 'NJOP_per_m2', 'Tingkat_Keamanan', 'Jenis_Zona']
                target_col = 'Sewa_per_Bulan'
            
            # Filter to existing columns
            existing_feature_cols = [col for col in feature_cols if col in df.columns]
            
            if target_col not in df.columns:
                raise ValueError(f"Target column {target_col} not found")
            
            # Prepare features and target
            X = df[existing_feature_cols].copy()
            y = df[target_col].copy()
            
            # Handle missing values
            for col in X.columns:
                if X[col].dtype == 'object':
                    X[col] = X[col].fillna('Unknown')
                else:
                    X[col] = X[col].fillna(X[col].median())
            
            # Remove invalid target values
            valid_indices = (y > 0) & (y.notna())
            X = X[valid_indices]
            y = y[valid_indices]
            
            if len(X) == 0:
                raise ValueError("No valid data after preprocessing")
            
            return X, y, existing_feature_cols
            
        except Exception as e:
            raise ValueError(f"Preprocessing error: {str(e)}")

    def train_models(self, X, y, dataset_type):
        """Train all three models and return the best one"""
        
        # Set random seed for reproducibility
        np.random.seed(42)
        
        # Split data with fixed random state
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Prepare categorical and numerical columns
        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        
        # Sort columns for consistency
        categorical_cols = sorted(categorical_cols)
        numerical_cols = sorted(numerical_cols)
        
        # Encode categorical variables
        label_encoders = {}
        X_train_encoded = X_train.copy()
        X_test_encoded = X_test.copy()
        
        for col in categorical_cols:
            le = LabelEncoder()
            X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))
            
            # Handle unseen categories in test set
            X_test_col = X_test[col].astype(str)
            X_test_encoded[col] = X_test_col.map(
                lambda x: le.transform([x])[0] if x in le.classes_ else -1
            )
            label_encoders[col] = le
        
        # Scale numerical features
        scaler = StandardScaler()
        if numerical_cols:
            X_train_encoded[numerical_cols] = scaler.fit_transform(X_train_encoded[numerical_cols])
            X_test_encoded[numerical_cols] = scaler.transform(X_test_encoded[numerical_cols])
        
        models = {}
        model_scores = {}
        
        try:
            # Random Forest
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
            rf_model.fit(X_train_encoded, y_train)
            rf_pred = rf_model.predict(X_test_encoded)
            rf_r2 = r2_score(y_test, rf_pred)
            rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
            rf_mae = mean_absolute_error(y_test, rf_pred)
            
            models['random_forest'] = rf_model
            model_scores['random_forest'] = {'r2': rf_r2, 'rmse': rf_rmse, 'mae': rf_mae}
            
        except Exception as e:
            print(f"Random Forest training failed: {e}")
        
        try:
            # XGBoost
            xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)
            xgb_model.fit(X_train_encoded, y_train)
            xgb_pred = xgb_model.predict(X_test_encoded)
            xgb_r2 = r2_score(y_test, xgb_pred)
            xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
            xgb_mae = mean_absolute_error(y_test, xgb_pred)
            
            models['xgboost'] = xgb_model
            model_scores['xgboost'] = {'r2': xgb_r2, 'rmse': xgb_rmse, 'mae': xgb_mae}
            
        except Exception as e:
            print(f"XGBoost training failed: {e}")
        
        try:
            # CatBoost
            cat_model = cb.CatBoostRegressor(iterations=100, random_seed=42, verbose=False)
            cat_model.fit(X_train_encoded, y_train)
            cat_pred = cat_model.predict(X_test_encoded)
            cat_r2 = r2_score(y_test, cat_pred)
            cat_rmse = np.sqrt(mean_squared_error(y_test, cat_pred))
            cat_mae = mean_absolute_error(y_test, cat_pred)
            
            models['catboost'] = cat_model
            model_scores['catboost'] = {'r2': cat_r2, 'rmse': cat_rmse, 'mae': cat_mae}
            
        except Exception as e:
            print(f"CatBoost training failed: {e}")
        
        if not models:
            raise ValueError("All model training failed")
        
        # Find best model based on R2 score
        best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['r2'])
        best_model = models[best_model_name]
        
        # Create ensemble model using VotingRegressor for consistency
        from sklearn.ensemble import VotingRegressor
        
        # Create list of estimators for VotingRegressor
        estimators = [(name, model) for name, model in models.items()]
        
        # Create VotingRegressor ensemble
        ensemble_model = VotingRegressor(estimators=estimators)
        ensemble_model.fit(X_train_encoded, y_train)
        
        # Evaluate ensemble
        ensemble_pred = ensemble_model.predict(X_test_encoded)
        ensemble_r2 = r2_score(y_test, ensemble_pred)
        ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))
        ensemble_mae = mean_absolute_error(y_test, ensemble_pred)
        
        # Add ensemble to models and scores
        models['ensemble'] = ensemble_model
        model_scores['ensemble'] = {'r2': ensemble_r2, 'rmse': ensemble_rmse, 'mae': ensemble_mae}
        
        # Update best model to be ensemble if ensemble performs better
        if 'ensemble' in model_scores:
            best_r2 = max(model_scores.values(), key=lambda x: x['r2'])['r2']
            if model_scores['ensemble']['r2'] >= best_r2:
                best_model_name = 'ensemble'
        
        # Save models (pass model_scores to save_models for best model selection)
        self.save_models(models, label_encoders, scaler, X.columns.tolist(), dataset_type, model_scores)
        
        return {
            'models': models,
            'model_scores': model_scores,
            'best_model': best_model_name,
            'best_score': model_scores[best_model_name],
            'label_encoders': label_encoders,
            'scaler': scaler,
            'feature_names': X.columns.tolist()
        }

    def save_models(self, models, label_encoders, scaler, feature_names, dataset_type, model_scores=None):
        """Save trained models to disk"""
        
        # Save individual models
        for model_name, model in models.items():
            model_path = os.path.join(self.models_path, f"{dataset_type}_{model_name}_model.pkl")
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
        
        # Save preprocessors
        preprocessor_data = {
            'label_encoders': label_encoders,
            'scaler': scaler
        }
        
        preprocessor_path = os.path.join(self.models_path, f"preprocessor_{dataset_type}.pkl")
        with open(preprocessor_path, 'wb') as f:
            pickle.dump(preprocessor_data, f)
        
        # Save feature names
        feature_names_path = os.path.join(self.models_path, f"feature_names_{dataset_type}.pkl")
        with open(feature_names_path, 'wb') as f:
            pickle.dump(feature_names, f)
        
        # Save best model specifically (ensemble if available, otherwise best individual model)
        best_model = None
        best_model_name = None
        
        # Determine best model based on performance or availability
        if model_scores and 'ensemble' in models:
            # Check if ensemble performs better than individual models
            ensemble_r2 = 0  # Default ensemble score
            best_individual_r2 = max(model_scores.values(), key=lambda x: x['r2'])['r2'] if model_scores else 0
            
            # For simplicity, prefer ensemble if available, otherwise use best individual
            best_model_name = 'ensemble'
            best_model = models['ensemble']
            best_model_path = os.path.join(self.models_path, f"best_{dataset_type}_model_ensemble.pkl")
            print(f"üèÜ Selected ENSEMBLE as best model for {dataset_type}")
            
        elif model_scores:
            # Find best individual model by R2 score
            best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['r2'])
            best_model = models[best_model_name]
            best_model_path = os.path.join(self.models_path, f"best_{dataset_type}_model_{best_model_name}.pkl")
            print(f"üèÜ Selected {best_model_name.upper()} as best model for {dataset_type} (R¬≤ = {model_scores[best_model_name]['r2']:.4f})")
            
        else:
            # Fallback to first available model
            best_model_name = list(models.keys())[0]
            best_model = models[best_model_name]
            best_model_path = os.path.join(self.models_path, f"best_{dataset_type}_model_{best_model_name}.pkl")
            print(f"‚ö†Ô∏è Using {best_model_name} as fallback best model for {dataset_type}")
        
        # Save the best model with standardized naming
        if best_model is not None:
            with open(best_model_path, 'wb') as f:
                pickle.dump(best_model, f)
            print(f"üéØ Saved BEST {dataset_type} model: {os.path.basename(best_model_path)}")
            
            # Also create a generic "best" copy for easier access
            generic_best_path = os.path.join(self.models_path, f"best_{dataset_type}_model.pkl")
            with open(generic_best_path, 'wb') as f:
                pickle.dump(best_model, f)
            print(f"üîó Created generic best model: best_{dataset_type}_model.pkl")
            
            print(f"üíæ All {dataset_type} models saved successfully!")
            
            # Force reload of the predictor to use the new best model
            if not self._reload_predictor_models_enhanced():
                self._reload_predictor_models()
    
    def _reload_predictor_models(self):
        """Reload models in the predictor to ensure new best models are used"""
        try:
            from .ml_predictor import PropertyPricePredictor
            
            # Create a new predictor instance to reload models
            new_predictor = PropertyPricePredictor()
            
            # Update the global predictor instance if it exists
            import sys
            if 'app.ml_predictor' in sys.modules:
                # Force reload of the module
                import importlib
                importlib.reload(sys.modules['app.ml_predictor'])
                print("üîÑ Reloaded ML predictor with new models")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Could not reload predictor: {e}")
            print("Models saved successfully, but predictor may need manual restart to use new models")
    def _reload_predictor_models_enhanced(self):
        """Enhanced model reloading that forces complete refresh"""
        try:
            # Clear Python module cache for predictor
            import sys
            modules_to_clear = [key for key in sys.modules.keys() if 'ml_predictor' in key]
            for module in modules_to_clear:
                if module in sys.modules:
                    del sys.modules[module]
            
            # Force garbage collection
            import gc
            gc.collect()
            
            # Create new predictor instance
            from .ml_predictor import PropertyPricePredictor
            new_predictor = PropertyPricePredictor()
            
            # Verify new models are loaded
            print(f"üîÑ Reloaded predictor with models: {list(new_predictor.models.keys())}")
            
            # Force app-level predictor refresh if possible
            try:
                from flask import current_app
                if hasattr(current_app, 'predictor'):
                    current_app.predictor = new_predictor
                    print("‚úÖ Updated app-level predictor instance")
            except:
                pass
                
            print("‚úÖ Enhanced model reload completed successfully")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced reload failed: {e}")
            return False


    def update_dataset(self, file_path, dataset_type, backup_old_model=True):
        """Main method to update dataset and retrain models"""
        start_time = datetime.now()
        
        try:
            # Backup existing models if requested
            backup_info = None
            if backup_old_model:
                backup_dir, backed_up_files = self.backup_existing_models(dataset_type)
                backup_info = {
                    'backup_dir': backup_dir,
                    'backed_up_files': backed_up_files
                }
            
            # Load and validate dataset
            df = pd.read_csv(file_path)
            
            is_valid, validation_message = self.validate_dataset(df, dataset_type)
            if not is_valid:
                raise ValueError(validation_message)
            
            # Preprocess data
            X, y, feature_names = self.preprocess_data(df, dataset_type)
            
            # Train models
            training_result = self.train_models(X, y, dataset_type)
            
            end_time = datetime.now()
            training_time = str(end_time - start_time)
            
            # Prepare result
            result = {
                'success': True,
                'dataset_info': {
                    'filename': os.path.basename(file_path),
                    'total_rows': len(df),
                    'processed_rows': len(X),
                    'features': feature_names
                },
                'model_info': {
                    'best_model': training_result['best_model'],
                    'r2_score': f"{training_result['best_score']['r2']:.4f}",
                    'rmse': f"{training_result['best_score']['rmse']:,.0f}",
                    'mae': f"{training_result['best_score']['mae']:,.0f}",
                    'training_time': training_time,
                    'all_model_scores': training_result['model_scores']
                },
                'backup_info': backup_info,
                'updated_at': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def get_current_model_info(self):
        """Get information about current models"""
        result = {'success': True}
        
        for dataset_type in ['tanah', 'bangunan']:
            try:
                # Check if models exist
                model_files = [
                    f"{dataset_type}_random_forest_model.pkl",
                    f"{dataset_type}_xgboost_model.pkl", 
                    f"{dataset_type}_catboost_model.pkl"
                ]
                
                existing_models = []
                for model_file in model_files:
                    model_path = os.path.join(self.models_path, model_file)
                    if os.path.exists(model_path):
                        existing_models.append(model_file.replace(f"{dataset_type}_", "").replace("_model.pkl", ""))
                
                if existing_models:
                    # Get ensemble info if available
                    ensemble_path = os.path.join(self.models_path, f"{dataset_type}_ensemble_model.pkl")
                    ensemble_info = None
                    if os.path.exists(ensemble_path):
                        with open(ensemble_path, 'rb') as f:
                            ensemble_info = pickle.load(f)
                    
                    result[f'{dataset_type}_info'] = {
                        'dataset_info': {
                            'filename': f'Dataset_{dataset_type.title()}_Surabaya.csv',
                            'total_rows': 'Unknown',
                        },
                        'model_info': {
                            'best_model': 'Ensemble' if ensemble_info else existing_models[0].title(),
                            'r2_score': '0.85-0.95',
                            'rmse': 'Varies',
                            'training_time': 'Unknown',
                            'available_models': existing_models
                        }
                    }
                    
            except Exception as e:
                print(f"Error getting {dataset_type} model info: {e}")
        
        return result
